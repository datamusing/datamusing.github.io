<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Musings in Amusing Data]]></title>
  <link href="http://datamusing.github.io/atom.xml" rel="self"/>
  <link href="http://datamusing.github.io/"/>
  <updated>2014-08-19T17:50:59-07:00</updated>
  <id>http://datamusing.github.io/</id>
  <author>
    <name><![CDATA[Sudeep Das]]></name>
    <email><![CDATA[datamusing@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[On a mission, in the Mission]]></title>
    <link href="http://datamusing.github.io/blog/2014/08/02/on-a-mission/"/>
    <updated>2014-08-02T10:17:34-07:00</updated>
    <id>http://datamusing.github.io/blog/2014/08/02/on-a-mission</id>
    <content type="html"><![CDATA[<h3 id="or-how-i-leveraged-machine-learning-to-find-must-try-dishes-in-my-neighborhood"><em>or, how I leveraged machine learning to find must try dishes in my neighborhood!</em></h3>

<p>SAN FRANCISCO, CA — Whether its the yummy <em>focaccia de recco</em> at <strong>Farina</strong>, the scrumptious <em>steak tartare</em> at <strong>Bar Tartine</strong>, the <em>chicken liver mousse</em> at Range, or the absolutely bulletproof <em>sesame fried chicken</em> at <strong>Foreign Cinema</strong> — each fine dining restaurant in the Mission District of San Francisco seems to have something mouthwateringly unique to offer.</p>

<p><img class="center" src="https://dl.dropboxusercontent.com/u/18915298/blog/mission_food/bar_tartine.jpg" /></p>

<p>The Mission is my neighborhood.  I endearingly call it  the “SF Foodie Central”, not just because some of the best fine dinning restaurants  happen to be here, but also because smaller, fiercely independent purveyors of fine delicacies like <em>Craftsman &amp; Wolves</em>, <em>Dandelion Chocolate</em>, <em>Tartine Bakery</em>, <em>Bi-Rite Creamery</em>,  <em>Four Barrel Coffee</em>, and <em>Ritual Roasters</em>  keep the food scene vibrant and at its delicious best! And then, there is the Mission Mexican food, which is hands down the best in the city. 
  <!--more--> </p>

<p>So it was quite predictable that when analyzing <a href="http://www.opentable.com">OpenTable</a> diner reviews for a data science concept project, I chose to go after the restaurants in the Mission District first. The result of this analysis is a list of <em>the most talked about dishes</em> in the restaurants. </p>

<p>The impatient readers who do not want the algorithm to stand between them and the dishes can scroll down straight to the bottom of the page where all will be revealed.  The inquisitive ones please read on. </p>

<h2 id="an-overview-of-the-algorithm">An overview of the algorithm</h2>

<p>Before I inundate you with names and pictures of those amazing dishes, let me get through the crux of the algorithm as quickly as possible. 
<img class="right" src="https://dl.dropboxusercontent.com/u/18915298/blog/mission_food/brunch.png" width="400" height="400" />
The first insight I wanted glean from the huge amount of reviews we have was the set of  main themes, or <em>topics</em> that people are talking about. That is done with a method called <em>topic analysis</em>, for which I used non-negative matrix factorization (NMF) as the main technique (I will write more about the nuances of the method in a future post).  A topic, is essentially a coherent collection of words - or a distribution of over words in the vocabulary. The top words in the <strong>brunch</strong> topic, e.g. may look like the one on the right. It is almost magical to see how words like  <em>brunch, sunday, eggs, benedict, omelette, poached, mimosa, hash, brown</em> and so forth come together into a tightly thematic topic that is readily interpretable. Topic analysis gave me topics for food, drinks, ambiance, service, and so on. For the menu item hunt that was the main focus of this post,  I concentrated on all the food topics. Note that the way topic analysis works, we obtain as a free byproduct the weight of the topics in each review, and by extension, the weight of each topic for each restaurant; e.g. the seafood topic will have very little weight in a restaurant that serves only meat dishes. Once I had that information, I could then look for the most frequently occurring words, bigrams, and trigrams within the context of the topics that peak at that restaurant, and quickly identify the most mentioned food items. There is some secret sauce that helps me disambiguate different ways reviewers talk about the same dish (e.g., someone says “cellophane noodles”, someone else says “cellophane noodles with crab”, yet another reviewer says “the crab noodles”, but they are all referring to the dish “cellophane noodles with dungeness crab”). 
In the end, you just count up the mappings to the dish item, rank them by number of mentions, and voila, you have the most mentioned dishes! </p>

<p>And now lets turn to the results! </p>

<h2 id="the-goods-delivered">The goods, delivered</h2>

<p>Although for each restaurant in the Mission, I could surface multiple dishes, I decided to only show the top (as in most mentioned) dish per restaurant. Note that this is very beta, and we can do so much more in terms of actually figuring out if the dish is being talked about in a positive manner and then count it in, etc. Those will come in due course! Meanwhile, here are the results (<em>scroll to center the iframe on your screen and scroll within the frame from one restaurant to the next</em>).  ENJOY! </p>

<iframe width="100%" height="768" frameborder="0" src="http://bl.ocks.org/anonymous/raw/dde021cbd234a015c111" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" oallowfullscreen="" msallowfullscreen=""></iframe>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Detecting people in photographs using skin tone]]></title>
    <link href="http://datamusing.github.io/blog/2014/07/06/detecting-people-in-photographs-using-skin-tone/"/>
    <updated>2014-07-06T00:26:21-07:00</updated>
    <id>http://datamusing.github.io/blog/2014/07/06/detecting-people-in-photographs-using-skin-tone</id>
    <content type="html"><![CDATA[<p>SAN FRANCISCO, CA — As a Data Scientist at <a href="http://opentable.com">OpenTable</a>, my computer screen often fills up with images of scrumptious food items (<em>effectively keeping my metabolic rate on  a high gear!</em>). Many of these photographs are professionally or semi-professionally taken by food photographers or enthusiasts (aka FoodSpotters!).  <img class="right" src="https://dl.dropboxusercontent.com/u/18915298/blog/detectSkin/tea-party.jpg" width="200" height="200" title="tea-party tea-party 2" /> Annoyingly, a lot of photos, especially those from social media channels,  come with portraits of eaters posing with the eaten, such as the one shown here.   Of course, one could use face detection algorithms, and other sophisticated techniques to weed out these photos. These techniques often require a lot of overhead and dependency on external libraries. Also, there may not even be a face in the photo to detect, but a hand or some portion of the torso might be showing. Over the weekend, I have been thinking about very fast ways of finding a human subject in a photograph, so that I could generate some quick features for classifying photos. It turns out that one promising way to do that could be to detect human skin pixels in the photos. </p>

<h2 id="detecting-skin-pixels"><em>Detecting skin pixels</em></h2>

<p>A quick digging into the subject of detecting skin pixels revealed a rich literature on this subject. As this is a blog post and not a review paper, I will only describe the bits I used, and leave the reader with this <a href="http://academic.aua.am/Skhachat/Public/Papers%20on%20Face%20Detection/Survey%20on%20Skin%20Color%20Techniques.pdf">paper</a> or <a href="http://pdf.aminer.org/000/367/151/image_chromatic_adaptation_using_anns_for_skin_color_adaptation.pdf">this one</a> as an entry point into this subject. <em>The fundamental concept behind pixel based skin detection is that the color of human skin (across various races and ethnicities) occupies a very tight region in the space of colors.</em>  In brief, there are three main ways to detect skin pixels: 
  <!--more--> </p>

<ol>
  <li><strong>Explicit Skin Model Based Method</strong>:  This class of methods try to use machine learning to find the best colorspace and a simple decision rule to define the boundaries the skin cluster in that colorspace. </li>
  <li><strong>Non-parametric Methods</strong>:  The key idea here is to estimate skin color distribution from a training data without deriving an explicit model of the skin color, e.g. a Naive-Bayes classifier. A skin/non-skin training data set can be found <a href="https://archive.ics.uci.edu/ml/datasets/Skin+Segmentation">here</a>.</li>
  <li><strong>Parametric Methods</strong>:  Here, one models the skin color distributions as parameterized probability distributions, such as Gaussians, or mixtures of Gaussians.</li>
</ol>

<p>As a first stab, I decided to go with method (1) above. A little more investigation led me to the  <a href="http://www.cse.unsw.edu.au/~tatjana/ICMLWS02/MLCV/Morales.pdf">Gomez and Morales (2002)</a> paper where the authors used a constructive induction algorithm which produces a single rule that defines the skin color boundary conditions in the RGB colorspace. Their method leads to an  extremely simple rule that goes as follows: </p>

<ul>
  <li>Extract R, G, B pixel values from the input image. </li>
  <li>Normalize:  <script type="math/tex">r=R/(R+G+B)</script>,   <script type="math/tex">g=G/(R+G+B)</script> and <script type="math/tex">b=B/(R+G+B)</script>, so that <script type="math/tex">(r+g+b) = 1</script> for each pixel.</li>
  <li>Next, generate three quantities (note that $(r+g+b)$ being unity is redundant here, but the authors probably left it in to make the normalization explicit): </li>
</ul>

<script type="math/tex; mode=display">% &lt;![CDATA[
  
\begin{eqnarray}
	\alpha &=& \frac{3b r^2}{(r+g+b)^3}\\
	\beta &=& \frac{r+g+b}{3r} + \frac{r-g}{r+g+b}	\\
	\gamma &=& \frac{r b+g^2}{g b}
\end{eqnarray}
 %]]&gt;</script>

<ul>
  <li>Finally, a pixel is categorized as “skin” if the pixel satisfies <strong>all</strong> of the following three conditions: </li>
</ul>

<p><script type="math/tex"> \alpha>0.1276</script>, <script type="math/tex">\beta \le 0.9498</script> and  <script type="math/tex">\gamma \le 2.7775 </script>.</p>

<p>Note that these rules were based on the training set available when the paper was written, and I should probably be regenerating the rules with more training examples available now. But, the color of skin has not changed significantly in 10 years, so lets continue! </p>

<p>Here is a quick Python implementation:</p>

<div class="bogus-wrapper"><notextile><figure class="code"><figcaption><span></span></figcaption><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
</pre></td><td class="code"><pre><code class="python"><span class="line"><span class="kn">import</span> <span class="nn">matplotlib.image</span> <span class="kn">as</span> <span class="nn">mpimg</span>
</span><span class="line"><span class="c"># read in the image</span>
</span><span class="line"><span class="n">values</span> <span class="o">=</span> <span class="n">mpimg</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s">&quot;tea-party.jpg&quot;</span><span class="p">)</span>
</span><span class="line"><span class="c"># separate out r, g, b channels</span>
</span><span class="line"><span class="n">r</span> <span class="o">=</span> <span class="n">values</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;f&#39;</span><span class="p">)</span>
</span><span class="line"><span class="n">g</span> <span class="o">=</span> <span class="n">values</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;f&#39;</span><span class="p">)</span>
</span><span class="line"><span class="n">b</span> <span class="o">=</span> <span class="n">values</span><span class="p">[:,:,</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">&#39;f&#39;</span><span class="p">)</span>
</span><span class="line"><span class="c"># generate the three quantities </span>
</span><span class="line"><span class="n">alpha</span> <span class="o">=</span> <span class="mi">3</span><span class="o">*</span><span class="n">b</span><span class="o">*</span><span class="n">r</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="p">(</span><span class="n">r</span><span class="o">+</span><span class="n">b</span><span class="o">+</span><span class="n">g</span><span class="p">)</span><span class="o">**</span><span class="mi">3</span>
</span><span class="line"><span class="n">beta</span> <span class="o">=</span>  <span class="p">(</span><span class="n">r</span><span class="o">+</span><span class="n">g</span><span class="o">+</span><span class="n">b</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="mi">3</span><span class="o">*</span><span class="n">r</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">r</span><span class="o">-</span><span class="n">g</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">r</span><span class="o">+</span><span class="n">g</span><span class="o">+</span><span class="n">b</span><span class="p">)</span>
</span><span class="line"><span class="n">gamma</span> <span class="o">=</span> <span class="p">(</span><span class="n">r</span><span class="o">*</span><span class="n">b</span><span class="o">+</span><span class="n">g</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">g</span><span class="o">*</span><span class="n">b</span><span class="p">)</span>
</span><span class="line"><span class="c"># finally we apply the rules:</span>
</span><span class="line"><span class="n">pylab</span><span class="o">.</span><span class="n">imshow</span><span class="p">((</span><span class="n">alpha</span><span class="o">&gt;</span><span class="mf">0.1276</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">beta</span><span class="o">&lt;=</span><span class="mf">0.9498</span><span class="p">)</span><span class="o">&amp;</span><span class="p">(</span><span class="n">gamma</span><span class="o">&lt;=</span><span class="mf">2.7775</span><span class="p">),</span><span class="n">cmap</span><span class="o">=</span><span class="s">&#39;gray&#39;</span><span class="p">)</span>
</span></code></pre></td></tr></table></div></figure></notextile></div>
<p>which gives us the following result, where white represents pixels labeled as skin (the original image is also below for easy comparison):<br />
<img class="center" src="https://dl.dropboxusercontent.com/u/18915298/blog/detectSkin/tea_party_binary.png" width="300" height="300" /></p>

<p><img class="center" src="https://dl.dropboxusercontent.com/u/18915298/blog/detectSkin/tea-party.jpg" width="300" height="300" /></p>

<p>As  you can see, this one simple rule does remarkably well in isolating skin pixels. </p>

<p>Here is another example:</p>

<p><img class="center" src="https://dl.dropboxusercontent.com/u/18915298/blog/detectSkin/slanted_door_comp.png" /></p>
]]></content>
  </entry>
  
</feed>
